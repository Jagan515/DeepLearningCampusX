{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd7f2cb",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras: A Beginner's Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome! This notebook is designed for beginners who want to dive into deep learning using Keras, a user-friendly API for building neural networks. As a deep learning engineer with 5 years of research experience, I've structured this guide to be clear, step-by-step, and practical. We'll start from the basics and build up to more advanced concepts.\n",
    "\n",
    "Deep learning is a subset of machine learning that uses neural networks with many layers to learn patterns from data. Keras makes it easy to prototype and experiment without getting bogged down in low-level details.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python knowledge (lists, functions, loops).\n",
    "- Familiarity with NumPy (for arrays) and Matplotlib (for plotting).\n",
    "- Install TensorFlow (which includes Keras): `pip install tensorflow`.\n",
    "\n",
    "We'll use simple examples, like classifying handwritten digits (MNIST dataset), to keep things hands-on.\n",
    "\n",
    "**Notebook Structure:**\n",
    "1. Neural Network Basics\n",
    "2. Setting Up Keras\n",
    "3. Building Your First Model\n",
    "4. Training and Evaluation\n",
    "5. Key Components: Activations, Losses, Optimizers\n",
    "6. Improving Models: Overfitting and Regularization\n",
    "7. Convolutional Neural Networks (CNNs)\n",
    "8. Recurrent Neural Networks (RNNs)\n",
    "9. Next Steps and Resources\n",
    "\n",
    "Run the cells as you go. Let's get started!\n",
    "\n",
    "## 1. Neural Network Basics\n",
    "\n",
    "A neural network mimics the human brain: it has **neurons** connected in **layers**. Data flows from input to output through these layers, adjusting \"weights\" during training to minimize errors.\n",
    "\n",
    "### Key Terms:\n",
    "- **Input Layer**: Takes raw data (e.g., pixel values of an image).\n",
    "- **Hidden Layers**: Process data (can be multiple).\n",
    "- **Output Layer**: Produces predictions (e.g., class labels).\n",
    "- **Forward Pass**: Data moves forward through the network.\n",
    "- **Backward Pass (Backpropagation)**: Errors flow backward to update weights.\n",
    "\n",
    "Imagine classifying a fruit: Input (color, size) → Hidden (features like \"red and round\") → Output (\"apple\").\n",
    "\n",
    "### Simple Math Behind It\n",
    "For a single neuron: Output = activation_function( sum(weights * inputs) + bias )\n",
    "\n",
    "We'll see this in code soon.\n",
    "\n",
    "## 2. Setting Up Keras\n",
    "\n",
    "Keras is now part of TensorFlow. Import it like this:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "```\n",
    "\n",
    "This sets up the environment. Keras uses \"Sequential\" models for simple stacked layers.\n",
    "\n",
    "## 3. Building Your First Model\n",
    "\n",
    "Let's build a basic neural network to classify MNIST digits (0-9). MNIST has 60,000 training images of 28x28 pixels.\n",
    "\n",
    "### Load and Prepare Data\n",
    "\n",
    "```python\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to 0-1 range (helps training)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for dense layers (flatten images)\n",
    "x_train = x_train.reshape(60000, 784)  # 28*28=784\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "```\n",
    "\n",
    "### Define the Model\n",
    "\n",
    "Use `Sequential` to stack layers. Start simple: Input → Dense (hidden) → Dense (output).\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(784,)),  # Hidden layer\n",
    "    keras.layers.Dense(10, activation='softmax')  # Output layer (10 classes)\n",
    "])\n",
    "\n",
    "model.summary()  # See the architecture\n",
    "```\n",
    "\n",
    "- `Dense`: Fully connected layer.\n",
    "- `relu`: Rectified Linear Unit (simple activation: max(0, x)).\n",
    "- `softmax`: For multi-class output (probabilities sum to 1).\n",
    "\n",
    "### Compile the Model\n",
    "\n",
    "Tell Keras how to train: loss function, optimizer, metrics.\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adaptive optimizer (good default)\n",
    "    loss='categorical_crossentropy',  # For multi-class\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "\n",
    "## 4. Training and Evaluation\n",
    "\n",
    "Train with `fit()`. Use validation data to monitor progress.\n",
    "\n",
    "```python\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,  # Process 128 samples at a time\n",
    "    epochs=5,  # Full passes over data\n",
    "    validation_split=0.2  # Use 20% of train for validation\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "```\n",
    "\n",
    "### Visualize Training\n",
    "\n",
    "Plot accuracy and loss to see if it's learning.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This should show improving accuracy. If validation lags, it's overfitting (we'll cover later).\n",
    "\n",
    "### Make Predictions\n",
    "\n",
    "```python\n",
    "# Predict on a single image\n",
    "sample = x_test[0:1]  # First test image\n",
    "prediction = model.predict(sample)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "print(f\"Predicted digit: {predicted_class}\")\n",
    "print(f\"Actual digit: {np.argmax(y_test[0])}\")\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(x_test[0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Predicted: {predicted_class}\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## 5. Key Components: Activations, Losses, Optimizers\n",
    "\n",
    "These are the building blocks.\n",
    "\n",
    "### Activations\n",
    "They introduce non-linearity. Common ones:\n",
    "\n",
    "- **ReLU**: Fast, avoids vanishing gradients. `activation='relu'`\n",
    "- **Sigmoid**: 0-1 output, for binary. `activation='sigmoid'`\n",
    "- **Softmax**: Multi-class probabilities.\n",
    "\n",
    "Example: Add more layers with different activations.\n",
    "\n",
    "```python\n",
    "# Deeper model\n",
    "model_v2 = keras.Sequential([\n",
    "    keras.layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_v2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "### Losses\n",
    "Measure prediction error.\n",
    "\n",
    "- **Categorical Crossentropy**: Multi-class classification.\n",
    "- **Binary Crossentropy**: Binary (0/1).\n",
    "- **MSE (Mean Squared Error)**: Regression (predict numbers).\n",
    "\n",
    "For regression example (predict house prices), use MSE.\n",
    "\n",
    "### Optimizers\n",
    "Update weights. \n",
    "\n",
    "- **SGD**: Basic stochastic gradient descent.\n",
    "- **Adam**: Adaptive rates, works well out-of-box.\n",
    "- **RMSprop**: Good for RNNs.\n",
    "\n",
    "Try SGD:\n",
    "\n",
    "```python\n",
    "model_v2.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train similarly...\n",
    "```\n",
    "\n",
    "## 6. Improving Models: Overfitting and Regularization\n",
    "\n",
    "Overfitting: Model memorizes training data but fails on new data (high train acc, low val acc).\n",
    "\n",
    "Solutions:\n",
    "\n",
    "### Dropout\n",
    "Randomly drop neurons during training.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model_reg = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    Dropout(0.2),  # Drop 20% of neurons\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_reg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_reg = model_reg.fit(x_train, y_train, epochs=5, validation_split=0.2)\n",
    "```\n",
    "\n",
    "### Other Techniques\n",
    "- **Early Stopping**: Stop if val loss doesn't improve.\n",
    "- **L1/L2 Regularization**: Penalize large weights.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# Use in fit()\n",
    "# history = model.fit(..., callbacks=[early_stop])\n",
    "```\n",
    "\n",
    "- **Data Augmentation**: For images, flip/rotate to create more data.\n",
    "\n",
    "## 7. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs excel at images by learning spatial features (edges, shapes).\n",
    "\n",
    "For MNIST, use Conv2D layers. Keep data as 28x28x1 (grayscale).\n",
    "\n",
    "```python\n",
    "# Reshape for CNN (channels last)\n",
    "x_train_cnn = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test_cnn = x_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "cnn_model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),  # Flatten to 1D\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_history = cnn_model.fit(x_train_cnn, y_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "test_acc_cnn = cnn_model.evaluate(x_test_cnn, y_test)[1]\n",
    "print(f\"CNN Test accuracy: {test_acc_cnn:.4f}\")\n",
    "```\n",
    "\n",
    "- **Conv2D**: Filters for features.\n",
    "- **MaxPooling**: Downsample, reduce params.\n",
    "- CNNs often beat dense nets on images.\n",
    "\n",
    "## 8. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs handle sequences (text, time series). They remember previous inputs.\n",
    "\n",
    "Simple LSTM (Long Short-Term Memory) for text classification (IMDB reviews).\n",
    "\n",
    "```python\n",
    "# Load IMDB data (binary sentiment: positive/negative)\n",
    "(x_train_seq, y_train_seq), (x_test_seq, y_test_seq) = keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "# Pad sequences to same length\n",
    "x_train_seq = keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen=200)\n",
    "x_test_seq = keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen=200)\n",
    "\n",
    "rnn_model = keras.Sequential([\n",
    "    keras.layers.Embedding(10000, 64),  # Word to vector\n",
    "    keras.layers.LSTM(64),  # LSTM layer\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "rnn_history = rnn_model.fit(x_train_seq, y_train_seq, epochs=5, batch_size=128, validation_split=0.2)\n",
    "\n",
    "test_acc_rnn = rnn_model.evaluate(x_test_seq, y_test_seq)[1]\n",
    "print(f\"RNN Test accuracy: {test_acc_rnn:.4f}\")\n",
    "```\n",
    "\n",
    "- **Embedding**: Converts words to dense vectors.\n",
    "- **LSTM**: Handles long dependencies better than basic RNN.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de23ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d381e3b4",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras:\n",
    "\n",
    "## 10. Transfer Learning\n",
    "\n",
    "Transfer learning is a powerful technique where you use a pre-trained model (trained on a large dataset like ImageNet) as a starting point for your own task. Instead of training from scratch, you \"transfer\" the learned features (e.g., edges, textures) to your problem. This saves time, data, and compute—ideal for beginners with limited resources.\n",
    "\n",
    "Why use it?\n",
    "- Pre-trained models like VGG16 or ResNet capture general image features.\n",
    "- Fine-tune the top layers for your specific classes (e.g., cats vs. dogs instead of 1,000 ImageNet classes).\n",
    "\n",
    "We'll use Keras's built-in pre-trained models from `keras.applications`. Example: Classify cats and dogs using a small dataset (you can download it from Kaggle, but we'll simulate loading).\n",
    "\n",
    "### Load Pre-trained Model and Data\n",
    "\n",
    "First, install if needed: `pip install tensorflow-datasets` (for easy data loading). But in code, we'll use a simple setup.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For this example, we'll use a subset of CIFAR-10 (animals-ish), but imagine cats/dogs.\n",
    "# Load CIFAR-10 for demo (10 classes, including cats/dogs)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Focus on binary: cat (class 3) vs dog (class 5)\n",
    "cat_dog_train = np.isin(y_train, [3, 5]).flatten()\n",
    "cat_dog_test = np.isin(y_test, [3, 5]).flatten()\n",
    "\n",
    "x_train_cd = x_train[cat_dog_train]\n",
    "y_train_cd = (y_train[cat_dog_train] == 5).astype(int)  # 0: cat, 1: dog\n",
    "x_test_cd = x_test[cat_dog_test]\n",
    "y_test_cd = (y_test[cat_dog_test] == 5).astype(int)\n",
    "\n",
    "# Normalize\n",
    "x_train_cd = x_train_cd.astype('float32') / 255.0\n",
    "x_test_cd = x_test_cd.astype('float32') / 255.0\n",
    "\n",
    "# One-hot for binary\n",
    "y_train_cd = keras.utils.to_categorical(y_train_cd, 2)\n",
    "y_test_cd = keras.utils.to_categorical(y_test_cd, 2)\n",
    "\n",
    "print(f\"Cat/Dog train samples: {x_train_cd.shape[0]}\")\n",
    "print(f\"Cat/Dog test samples: {x_test_cd.shape[0]}\")\n",
    "```\n",
    "\n",
    "### Build Transfer Learning Model\n",
    "\n",
    "Load VGG16 (pre-trained on ImageNet), freeze base layers, add custom top.\n",
    "\n",
    "```python\n",
    "# Load pre-trained VGG16 without top layers\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',  # Pre-trained weights\n",
    "    include_top=False,   # No classification head\n",
    "    input_shape=(32, 32, 3)  # CIFAR size\n",
    ")\n",
    "\n",
    "# Freeze base model (don't train these weights initially)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers on top\n",
    "model_tl = keras.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.GlobalAveragePooling2D(),  # Pool features\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2, activation='softmax')  # Binary output\n",
    "])\n",
    "\n",
    "model_tl.summary()  # See the huge base + small top\n",
    "```\n",
    "\n",
    "### Compile, Train, and Evaluate\n",
    "\n",
    "```python\n",
    "model_tl.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train (quick because base is frozen)\n",
    "history_tl = model_tl.fit(\n",
    "    x_train_cd, y_train_cd,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_tl.evaluate(x_test_cd, y_test_cd)\n",
    "print(f\"Transfer Learning Test accuracy: {test_acc:.4f}\")\n",
    "```\n",
    "\n",
    "### Fine-Tuning (Optional Advanced Step)\n",
    "\n",
    "Unfreeze some layers for better accuracy.\n",
    "\n",
    "```python\n",
    "# Unfreeze last few layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-4]:  # Freeze all but last 4\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model_tl.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Small LR to avoid destroying weights\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "history_fine = model_tl.fit(\n",
    "    x_train_cd, y_train_cd,\n",
    "    epochs=3,  # Fewer epochs\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n",
    "```\n",
    "\n",
    "**Pro Tip:** For real datasets, use `ImageDataGenerator` for augmentation. Transfer learning often gets 90%+ accuracy with little data.\n",
    "\n",
    "### Visualize a Prediction\n",
    "\n",
    "```python\n",
    "# Predict one image\n",
    "sample_img = x_test_cd[0:1]\n",
    "pred = model_tl.predict(sample_img)\n",
    "pred_class = np.argmax(pred)\n",
    "\n",
    "label_map = {0: 'Cat', 1: 'Dog'}\n",
    "print(f\"Predicted: {label_map[pred_class]}\")\n",
    "print(f\"Actual: {label_map[np.argmax(y_test_cd[0])]}\")\n",
    "\n",
    "plt.imshow(sample_img[0])\n",
    "plt.title(f\"Predicted: {label_map[pred_class]}\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## 11. Autoencoders\n",
    "\n",
    "Autoencoders are unsupervised neural networks that learn to compress (encode) and reconstruct (decode) data. They're great for tasks like denoising images, anomaly detection, or dimensionality reduction (like PCA but non-linear).\n",
    "\n",
    "Structure:\n",
    "- **Encoder**: Compresses input to a low-dimensional \"latent space\".\n",
    "- **Decoder**: Reconstructs from latent space back to original.\n",
    "- Train to minimize reconstruction error (e.g., MSE).\n",
    "\n",
    "We'll build a simple autoencoder to denoise MNIST digits.\n",
    "\n",
    "### Prepare Noisy Data\n",
    "\n",
    "```python\n",
    "# Reload MNIST\n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape to (samples, 28, 28, 1)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Add noise\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(0, 1, x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(0, 1, x_test.shape)\n",
    "\n",
    "# Clip to 0-1\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "print(f\"Noisy train shape: {x_train_noisy.shape}\")\n",
    "```\n",
    "\n",
    "### Build Autoencoder Model\n",
    "\n",
    "```python\n",
    "# Encoder\n",
    "encoder = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same')\n",
    "])  # Latent space: ~7x7x64\n",
    "\n",
    "# Decoder\n",
    "decoder = keras.Sequential([\n",
    "    keras.layers.Conv2DTranspose(64, (3, 3), activation='relu', strides=2, padding='same'),\n",
    "    keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=2, padding='same'),\n",
    "    keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')\n",
    "])\n",
    "\n",
    "# Full autoencoder\n",
    "autoencoder = keras.Sequential([encoder, decoder])\n",
    "\n",
    "autoencoder.summary()\n",
    "```\n",
    "\n",
    "### Compile, Train, and Denoise\n",
    "\n",
    "```python\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')  # Or MSE\n",
    "\n",
    "# Train on noisy, reconstruct clean\n",
    "history_ae = autoencoder.fit(\n",
    "    x_train_noisy, x_train,  # Input noisy, target clean\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test_noisy, x_test)\n",
    ")\n",
    "\n",
    "# Denoise test images\n",
    "denoised = autoencoder.predict(x_test_noisy)\n",
    "\n",
    "# Plot original, noisy, denoised\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n):\n",
    "    # Original\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(x_test[i].squeeze(), cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Noisy\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(x_test_noisy[i].squeeze(), cmap='gray')\n",
    "    plt.title('Noisy')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Denoised\n",
    "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
    "    plt.imshow(denoised[i].squeeze(), cmap='gray')\n",
    "    plt.title('Denoised')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Key Insight:** The latent space (encoder output) captures essential features. Extract it with `encoder.predict()` for compression tasks.\n",
    "\n",
    "### Variations\n",
    "- **Variational Autoencoder (VAE)**: Adds probabilistic sampling for generative models (like blurry image generation).\n",
    "- Use for anomalies: Train on normal data; high reconstruction error flags outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9befdf41",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras:\n",
    "\n",
    "## 13. Generative Adversarial Networks (GANs)\n",
    "\n",
    "GANs are a type of generative model where two neural networks compete: the **Generator** creates fake data, and the **Discriminator** tries to spot fakes. They \"adversarially\" train each other—the generator gets better at fooling the discriminator, and the discriminator gets sharper. Result? Realistic synthetic data, like fake images or text.\n",
    "\n",
    "Why learn GANs?\n",
    "- Fun for creating art or data augmentation.\n",
    "- Applications: Deepfakes, style transfer, super-resolution.\n",
    "- Challenge: Training can be unstable (mode collapse), but start simple.\n",
    "\n",
    "We'll build a basic DCGAN (Deep Convolutional GAN) to generate MNIST-like digits. Uses CNNs for images.\n",
    "\n",
    "### Prepare Data\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess MNIST\n",
    "(x_train, _), _ = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_train = (x_train - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "\n",
    "# Batch and shuffle\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "```\n",
    "\n",
    "### Build the Generator\n",
    "\n",
    "Takes random noise (latent vector) and outputs a fake image.\n",
    "\n",
    "```python\n",
    "def make_generator_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))  # Noise dim=100\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(keras.layers.Reshape((7, 7, 256)))\n",
    "\n",
    "    # Upsample to 14x14\n",
    "    model.add(keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "    # Upsample to 28x28\n",
    "    model.add(keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(keras.layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "generator.summary()\n",
    "```\n",
    "\n",
    "- `Conv2DTranspose`: Upsamples (reverse of pooling).\n",
    "- `BatchNormalization`: Stabilizes training.\n",
    "- `LeakyReLU`: Allows small negative values to avoid dying ReLUs.\n",
    "\n",
    "### Build the Discriminator\n",
    "\n",
    "Classifies real vs. fake images.\n",
    "\n",
    "```python\n",
    "def make_discriminator_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
    "    model.add(keras.layers.LeakyReLU())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(keras.layers.LeakyReLU())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()\n",
    "```\n",
    "\n",
    "- Outputs a scalar: ~1 for real, ~0 for fake (no sigmoid—use BCE loss).\n",
    "\n",
    "### Loss and Optimizers\n",
    "\n",
    "Binary crossentropy for both. Generator fools discriminator (wants \"real\" label for fakes).\n",
    "\n",
    "```python\n",
    "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = keras.optimizers.Adam(1e-4)\n",
    "```\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "Keras doesn't have built-in GAN training, so we write a custom loop.\n",
    "\n",
    "```python\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# Sample noise for fixed generations\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "        if epoch % 10 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "        # Generate samples every epoch\n",
    "        predictions = generator(seed, training=False)\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        for i in range(predictions.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.savefig(f'GAN_image_at_epoch_{epoch+1:04d}.png')\n",
    "        plt.show()\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "# Run training (this takes time!)\n",
    "# train(train_dataset, EPOCHS)\n",
    "```\n",
    "\n",
    "**Note:** Uncomment the last line to train. Expect ~30-50 epochs for decent digits. Monitor losses: They should balance (not one dominating).\n",
    "\n",
    "**Tips:** If unstable, lower learning rates or add noise to inputs. For colors, use Fashion-MNIST or CelebA.\n",
    "\n",
    "## 14. Transformers\n",
    "\n",
    "Transformers revolutionized NLP (and vision) with \"attention\" mechanisms—no more RNNs for sequences! They process entire inputs in parallel, focusing on relevant parts via self-attention.\n",
    "\n",
    "Key Idea: **Attention** computes how much each word/token \"attends\" to others. Stacked encoder-decoder blocks.\n",
    "\n",
    "In Keras, use `MultiHeadAttention` for custom models. Example: Simple text classifier on IMDB (sentiment).\n",
    "\n",
    "### Prepare Data\n",
    "\n",
    "```python\n",
    "# Reload IMDB\n",
    "max_features = 10000  # Vocab size\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n",
    "sequence_length = 250\n",
    "\n",
    "# Pad/truncate\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=sequence_length)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=sequence_length)\n",
    "\n",
    "vocab = keras.datasets.imdb.get_word_index()\n",
    "reverse_vocab = {value + 3: key for key, value in vocab.items()}\n",
    "reverse_vocab[0] = '[PAD]'\n",
    "reverse_vocab[1] = '[UNK]'\n",
    "reverse_vocab[2] = '[START]'\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    return ' '.join([reverse_vocab.get(i, '?') for i in input_seq])\n",
    "\n",
    "print(decode_sequence(x_train[0]))  # Sample review\n",
    "```\n",
    "\n",
    "### Build Transformer Model\n",
    "\n",
    "Embeddings + positional encoding + attention layers.\n",
    "\n",
    "```python\n",
    "embed_dim = 32  # Embedding size\n",
    "num_heads = 2   # Attention heads\n",
    "ff_dim = 32     # Feed-forward dim\n",
    "num_classes = 2 # Binary\n",
    "\n",
    "inputs = keras.Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding_layer = keras.layers.Embedding(max_features, embed_dim)(inputs)\n",
    "# Positional encoding (simple: add positions)\n",
    "positions = tf.range(start=0, limit=sequence_length, delta=1)\n",
    "pos_embedding = keras.layers.Embedding(sequence_length, embed_dim)(positions)\n",
    "x = embedding_layer + pos_embedding\n",
    "\n",
    "# Transformer block\n",
    "attention_output = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "attention_output = keras.layers.Dropout(0.1)(attention_output)\n",
    "out1 = keras.layers.LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "\n",
    "ff_out = keras.layers.Dense(ff_dim, activation='relu')(out1)\n",
    "ff_out = keras.layers.Dense(embed_dim)(ff_out)\n",
    "ff_out = keras.layers.Dropout(0.1)(ff_out)\n",
    "out2 = keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ff_out)\n",
    "\n",
    "# Global pooling and classify\n",
    "pooled = keras.layers.GlobalAveragePooling1D()(out2)\n",
    "outputs = keras.layers.Dense(num_classes, activation='softmax')(pooled)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### Train and Evaluate\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Transformer Test accuracy: {test_acc:.4f}\")\n",
    "```\n",
    "\n",
    "- Beats simple LSTM? Often yes, due to parallel processing.\n",
    "- For generation (e.g., GPT-like), add decoder stacks.\n",
    "\n",
    "**Advanced:** Use Keras's `TransformerEncoder` for easier stacking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4b345",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras: A Beginner's Guide (Continued)\n",
    "\n",
    "## 16. Diffusion Models\n",
    "\n",
    "Diffusion models are a newer class of generative models that have taken the AI world by storm (think DALL-E or Stable Diffusion). They work by gradually adding noise to data (forward diffusion) and then learning to reverse it (denoising) to generate new samples. Unlike GANs, they're more stable to train but require more compute.\n",
    "\n",
    "Why learn them?\n",
    "- Excellent for high-quality image/text-to-image generation.\n",
    "- Probabilistic: Generate diverse outputs.\n",
    "- Beginner tip: Start with simple unconditional generation on MNIST.\n",
    "\n",
    "We'll implement a basic Denoising Diffusion Probabilistic Model (DDPM) using Keras. This is simplified—real ones use U-Nets.\n",
    "\n",
    "### Prepare Data and Setup\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load MNIST\n",
    "(x_train, _), _ = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_train = np.expand_dims(x_train, -1)  # Add channel dim: (60000, 28, 28, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "timesteps = 1000  # Diffusion steps\n",
    "batch_size = 64\n",
    "img_shape = x_train.shape[1:]  # (28, 28, 1)\n",
    "```\n",
    "\n",
    "### Diffusion Process\n",
    "\n",
    "Forward: Add Gaussian noise over T steps. We predict noise at each step.\n",
    "\n",
    "Define beta schedule (noise variance).\n",
    "\n",
    "```python\n",
    "def linear_beta_schedule(timesteps):\n",
    "    return np.linspace(1e-4, 0.02, timesteps)\n",
    "\n",
    "betas = linear_beta_schedule(timesteps)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "\n",
    "# Helper to add noise\n",
    "def add_noise(x_start, t, noise):\n",
    "    sqrt_alphas_cumprod_t = tf.gather(alphas_cumprod, t)\n",
    "    sqrt_one_minus_alphas_cumprod_t = tf.sqrt(1. - sqrt_alphas_cumprod_t)\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "# Sample random timesteps and noise\n",
    "def sample_timesteps(batch_size):\n",
    "    return tf.random.uniform((batch_size,), 0, timesteps, dtype=tf.int32)\n",
    "\n",
    "def sample_noise(batch_size, img_shape):\n",
    "    return tf.random.normal((batch_size,) + img_shape)\n",
    "```\n",
    "\n",
    "### Build the Denoising Model (U-Net like, Simple)\n",
    "\n",
    "A small CNN that predicts noise given noisy image and timestep.\n",
    "\n",
    "```python\n",
    "def build_model(img_shape, timesteps):\n",
    "    inputs = layers.Input(shape=img_shape)\n",
    "    t_input = layers.Input(shape=(), dtype=tf.int32)  # Timestep\n",
    "\n",
    "    # Embed timestep\n",
    "    max_encodings = 1000\n",
    "    t_emb = layers.Embedding(max_encodings, 32)(t_input)\n",
    "    t_emb = layers.Dense(128)(t_emb)\n",
    "\n",
    "    # Simple U-Net: downsample, upsample\n",
    "    h = layers.Conv2D(32, 3, padding='same')(inputs)\n",
    "    h = layers.ReLU()(h)\n",
    "    h = layers.Concatenate()([h, t_emb])  # Add time info (broadcast)\n",
    "\n",
    "    h = layers.Conv2D(64, 3, strides=2, padding='same')(h)\n",
    "    h = layers.ReLU()(h)\n",
    "\n",
    "    h = layers.Conv2DTranspose(32, 3, strides=2, padding='same')(h)\n",
    "    h = layers.ReLU()(h)\n",
    "\n",
    "    # Predict noise\n",
    "    output = layers.Conv2D(1, 3, padding='same')(h)\n",
    "\n",
    "    model = keras.Model([inputs, t_input], output)\n",
    "    return model\n",
    "\n",
    "model = build_model(img_shape, timesteps)\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "Train to predict added noise.\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.Adam(1e-4)\n",
    "mse_loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x):\n",
    "    noise = sample_noise(batch_size, img_shape)\n",
    "    t = sample_timesteps(batch_size)\n",
    "    \n",
    "    # Add noise\n",
    "    x_noisy = add_noise(x, t, noise)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_noise = model([x_noisy, t], training=True)\n",
    "        loss = mse_loss(noise, predicted_noise)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training (simplified; run for epochs)\n",
    "epochs = 10\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(60000).batch(batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataset:\n",
    "        loss = train_step(batch)\n",
    "        total_loss += loss\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataset):.4f}\")\n",
    "```\n",
    "\n",
    "### Sampling (Generation)\n",
    "\n",
    "Start from pure noise, iteratively denoise.\n",
    "\n",
    "```python\n",
    "def sample(model, num_samples, timesteps):\n",
    "    # Start with noise\n",
    "    x = tf.random.normal((num_samples, *img_shape))\n",
    "    \n",
    "    for t in reversed(range(timesteps)):\n",
    "        t_batch = tf.fill([num_samples], t)\n",
    "        \n",
    "        # Predict noise\n",
    "        pred_noise = model([x, t_batch], training=False)\n",
    "        \n",
    "        # Remove noise (simplified DDPM step)\n",
    "        alpha_t = alphas[t]\n",
    "        beta_t = betas[t]\n",
    "        if t > 0:\n",
    "            noise = tf.random.normal(tf.shape(x))\n",
    "        else:\n",
    "            noise = tf.zeros(tf.shape(x))\n",
    "        \n",
    "        x = (1 / tf.sqrt(alpha_t)) * (x - (beta_t / tf.sqrt(1 - alphas_cumprod[t])) * pred_noise) + tf.sqrt(beta_t) * noise\n",
    "    \n",
    "    x = tf.sigmoid(x)  # To [0,1]\n",
    "    return x\n",
    "\n",
    "# Generate samples\n",
    "generated = sample(model, 16, timesteps)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(generated[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Note:** This is a toy example—train longer for better results. For text-to-image, add conditioning (e.g., CLIP embeddings).\n",
    "\n",
    "## 17. Model Deployment\n",
    "\n",
    "Deployment means taking your trained model from notebook to production: save it, serve predictions via API, or run on mobile/web. Keras makes this easy with saving/loading and TensorFlow Serving.\n",
    "\n",
    "Why deploy?\n",
    "- Share models (e.g., via web app).\n",
    "- Real-time inference (e.g., image classifier API).\n",
    "\n",
    "We'll cover saving, loading, and a simple Flask API for serving.\n",
    "\n",
    "### Saving and Loading Models\n",
    "\n",
    "```python\n",
    "# Assume you have a trained model (e.g., from earlier MNIST CNN)\n",
    "# Quick recap: Simple CNN\n",
    "cnn_model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train briefly (use reshaped data)\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "cnn_model.fit(x_train, y_train, epochs=2, batch_size=128)\n",
    "\n",
    "# Save the entire model\n",
    "cnn_model.save('mnist_model.h5')  # HDF5 format\n",
    "\n",
    "# Or save in SavedModel format (for TF Serving)\n",
    "cnn_model.save('mnist_savedmodel', save_format='tf')\n",
    "```\n",
    "\n",
    "Load and use:\n",
    "\n",
    "```python\n",
    "# Load HDF5\n",
    "loaded_model_h5 = keras.models.load_model('mnist_model.h5')\n",
    "\n",
    "# Load SavedModel\n",
    "loaded_model_sm = keras.models.load_model('mnist_savedmodel')\n",
    "\n",
    "# Predict\n",
    "sample_img = x_train[0:1]\n",
    "pred = loaded_model_h5.predict(sample_img)\n",
    "print(f\"Predicted class: {np.argmax(pred)}\")\n",
    "```\n",
    "\n",
    "### Deploy with Flask API\n",
    "\n",
    "Install Flask: `pip install flask` (outside notebook). Create a simple server.\n",
    "\n",
    "Save this as `app.py`:\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = tf.keras.models.load_model('mnist_model.h5')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json['image']  # Expect 784-flatten list\n",
    "    img = np.array(data).reshape(1, 28, 28, 1).astype('float32') / 255.0\n",
    "    pred = model.predict(img)\n",
    "    return jsonify({'prediction': int(np.argmax(pred))})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "Test with curl or Postman: Send JSON `{\"image\": [0.1, 0.2, ...]}` to `http://localhost:5000/predict`.\n",
    "\n",
    "For production: Use TensorFlow Serving (`docker run -p 8501:8501 --mount type=bind,source=/path/to/model,target=/models/mnist -e MODEL_NAME=mnist -t tensorflow/serving`).\n",
    "\n",
    "**Mobile/Web:** Convert to TensorFlow Lite for apps: `converter = tf.lite.TFLiteConverter.from_keras_model(model); tflite_model = converter.convert(); open('model.tflite', 'wb').write(tflite_model)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0678c14",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras:\n",
    "\n",
    "## 19. Reinforcement Learning (RL) with Keras\n",
    "\n",
    "Reinforcement Learning is a paradigm where an agent learns by interacting with an environment, receiving rewards or penalties for actions. Unlike supervised learning (labeled data), RL is trial-and-error: maximize cumulative reward over time. Deep RL combines neural networks (like in Keras) for complex state-action spaces, powering games (AlphaGo) or robotics.\n",
    "\n",
    "Why RL?\n",
    "- Handles sequential decisions (e.g., playing CartPole).\n",
    "- Key concepts: State (observation), Action, Reward, Policy (what to do).\n",
    "- Beginner start: Use OpenAI Gym (now Gymnasium) environments.\n",
    "\n",
    "We'll build a simple Deep Q-Network (DQN) for CartPole: Balance a pole on a cart by moving left/right. Keras for the Q-network (predicts action values).\n",
    "\n",
    "### Setup and Environment\n",
    "\n",
    "Install Gymnasium: `pip install gymnasium[box2d]` (for physics). But in notebook, assume it's ready.\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]  # 4 (cart pos, vel, pole angle, ang vel)\n",
    "action_size = env.action_space.n  # 2 (left/right)\n",
    "\n",
    "print(f\"State size: {state_size}, Action size: {action_size}\")\n",
    "```\n",
    "\n",
    "### Replay Buffer (Memory)\n",
    "\n",
    "Store experiences (state, action, reward, next_state) for off-policy learning (replay random batches).\n",
    "\n",
    "```python\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[idx] for idx in batch])\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "```\n",
    "\n",
    "### DQN Model\n",
    "\n",
    "Neural net approximates Q-values: Q(s, a) = expected future reward from state s taking action a.\n",
    "\n",
    "```python\n",
    "def build_model(state_size, action_size):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(24, input_dim=state_size, activation='relu'),\n",
    "        layers.Dense(24, activation='relu'),\n",
    "        layers.Dense(action_size, activation='linear')  # Q-values for each action\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "model = build_model(state_size, action_size)\n",
    "target_model = build_model(state_size, action_size)  # Fixed target for stability\n",
    "target_model.set_weights(model.get_weights())\n",
    "```\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "Epsilon-greedy exploration: Random actions with prob epsilon (decay over time).\n",
    "\n",
    "```python\n",
    "def epsilon_greedy_action(state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample()  # Random\n",
    "    q_values = model.predict(state[np.newaxis], verbose=0)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "def train_step(batch_size=32, gamma=0.95):  # Discount factor\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    # Q-targets: r + gamma * max Q(next_s, a') if not done\n",
    "    targets = model.predict(states, verbose=0)\n",
    "    next_q_values = model.predict(next_states, verbose=0)\n",
    "    target_q_values = target_model.predict(next_states, verbose=0)\n",
    "    for i in range(batch_size):\n",
    "        if dones[i]:\n",
    "            targets[i][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            targets[i][actions[i]] = rewards[i] + gamma * np.max(target_q_values[i])\n",
    "    \n",
    "    model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "# Main training\n",
    "episodes = 500\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        train_step()\n",
    "    \n",
    "    rewards_history.append(total_reward)\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Update target every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode}, Epsilon: {epsilon:.2f}, Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n",
    "```\n",
    "\n",
    "### Evaluation and Plot\n",
    "\n",
    "Test without exploration.\n",
    "\n",
    "```python\n",
    "# Test episode\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "test_rewards = []\n",
    "while not done:\n",
    "    q_values = model.predict(state[np.newaxis], verbose=0)\n",
    "    action = np.argmax(q_values[0])\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    test_rewards.append(reward)\n",
    "\n",
    "print(f\"Test episode reward: {sum(test_rewards)}\")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.plot(rewards_history)\n",
    "plt.title('Training Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Insights:** CartPole solves at 200+ reward. If unstable, tune gamma or buffer size. For advanced: Double DQN or Atari games.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21cdfe",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras: \n",
    "\n",
    "## 21. Federated Learning\n",
    "\n",
    "Federated Learning (FL) trains models across decentralized devices (e.g., phones) without sharing raw data—only model updates (gradients) are sent to a central server. This preserves privacy (GDPR-friendly) and works on edge devices. In Keras/TensorFlow, use TensorFlow Federated (TFF) for simulation.\n",
    "\n",
    "Why FL?\n",
    "- Privacy: Data stays local (e.g., keyboard predictions on your phone).\n",
    "- Efficiency: Reduces bandwidth (no full datasets uploaded).\n",
    "- Beginner note: Simulate with TFF on a single machine—real FL needs distributed setup.\n",
    "\n",
    "We'll simulate FL on MNIST: \"Clients\" (simulated users) train locally, server aggregates.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Install TFF: `pip install tensorflow-federated` (outside notebook). TFF builds on Keras.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Enable TFF logging (optional)\n",
    "tff.framework.set_default_executor(tff.framework.LocalPythonExecutor())\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"TFF version: {tff.__version__}\")\n",
    "```\n",
    "\n",
    "### Prepare Federated Data\n",
    "\n",
    "TFF has built-in loaders. Split MNIST into client simulations (e.g., 10 clients, uneven data).\n",
    "\n",
    "```python\n",
    "# Load MNIST as federated data\n",
    "def preprocess(dataset):\n",
    "    def element_fn(element):\n",
    "        return (tf.expand_dims(element['pixels'], -1) / 255.0, element['label'])\n",
    "    return dataset.map(element_fn).batch(20)  # Batch size\n",
    "\n",
    "NUM_CLIENTS = 10\n",
    "train, test = tff.simulation.datasets.emnist.load_data(num_clients=NUM_CLIENTS, only_digits=True)\n",
    "\n",
    "# Sample clients\n",
    "train_federated = train.preprocess(preprocess)\n",
    "test_federated = test.preprocess(preprocess)\n",
    "\n",
    "# Create sample batches for inspection\n",
    "sample_client_data = next(iter(train_federated.create_tf_dataset_for_client(train_federated.client_ids[0])))\n",
    "print(f\"Sample client batch shape: {sample_client_data[0].shape}\")\n",
    "```\n",
    "\n",
    "### Define the Model\n",
    "\n",
    "A simple Keras model, wrapped for TFF.\n",
    "\n",
    "```python\n",
    "def create_keras_model():\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Dense(10, activation='relu', input_shape=(784,)),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# TFF model function (must be stateless)\n",
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=sample_client_data[0].shape,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "```\n",
    "\n",
    "### Federated Averaging (FedAvg)\n",
    "\n",
    "TFF's algorithm: Local training + server average.\n",
    "\n",
    "```python\n",
    "# Compile and train\n",
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn)\n",
    "\n",
    "state = iterative_process.initialize()\n",
    "NUM_ROUNDS = 10\n",
    "\n",
    "for round_num in range(1, NUM_ROUNDS + 1):\n",
    "    state, metrics = iterative_process.next(state, train_federated)\n",
    "    print(f'Round {round_num}, Metrics: {metrics}')\n",
    "```\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Aggregate test metrics centrally.\n",
    "\n",
    "```python\n",
    "# Get central evaluation\n",
    "evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
    "test_metrics = evaluation(state.model, test_federated)\n",
    "print(f\"Federated Test Metrics: {test_metrics}\")\n",
    "```\n",
    "\n",
    "**Key Differences from Central Training:**\n",
    "- No data centralization.\n",
    "- Heterogeneity: Clients may have different data distributions (non-IID).\n",
    "\n",
    "**Tips:** For real devices, use TFLite for on-device training. Handle stragglers with async FL.\n",
    "\n",
    "## 22. Explainable AI (XAI)\n",
    "\n",
    "Explainable AI makes \"black-box\" models interpretable—why did it predict that? Crucial for trust in healthcare/finance. Keras integrates with libraries like SHAP or LIME for post-hoc explanations.\n",
    "\n",
    "Why XAI?\n",
    "- Debug models (e.g., bias detection).\n",
    "- Regulatory: \"Right to explanation\" laws.\n",
    "- Beginner: Focus on SHAP (SHapley Additive exPlanations)—game theory for feature importance.\n",
    "\n",
    "Example: Explain MNIST predictions with SHAP.\n",
    "\n",
    "### Setup SHAP\n",
    "\n",
    "Install: `pip install shap`.\n",
    "\n",
    "```python\n",
    "import shap\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Quick MNIST model (from earlier)\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "model_xai = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_xai.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_xai.fit(x_train, y_train, epochs=2, verbose=0)  # Quick train\n",
    "```\n",
    "\n",
    "### SHAP Explanations\n",
    "\n",
    "Use GradientExplainer for deep nets.\n",
    "\n",
    "```python\n",
    "# Background data (subset for speed)\n",
    "background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n",
    "\n",
    "# Explainer\n",
    "explainer = shap.GradientExplainer(model_xai, background)\n",
    "\n",
    "# Explain test samples\n",
    "test_samples = x_test[:50]\n",
    "shap_values = explainer.shap_values(test_samples)\n",
    "\n",
    "# Plot for one sample\n",
    "sample_idx = 0\n",
    "shap.image_plot(shap_values[sample_idx], test_samples[sample_idx:1].reshape(1, 28, 28), show=False)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Red: Features increasing prediction.\n",
    "- Blue: Decreasing.\n",
    "- For MNIST: Highlights digit shapes.\n",
    "\n",
    "### LIME Alternative (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "Approximates locally with simple models.\n",
    "\n",
    "```python\n",
    "import lime\n",
    "import lime.lime_tabular as lime_tab\n",
    "\n",
    "# LIME for tabular (flatten images)\n",
    "explainer_lime = lime_tab.LimeTabularExplainer(\n",
    "    training_data=np.array(x_train),\n",
    "    feature_names=[f'pixel_{i}' for i in range(784)],\n",
    "    mode='classification',\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "# Explain one prediction\n",
    "exp = explainer_lime.explain_instance(\n",
    "    x_test[0], model_xai.predict, num_features=10\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)\n",
    "```\n",
    "\n",
    "**Global Insights:** Use SHAP summary plots for overall feature importances.\n",
    "\n",
    "**Pro Tip:** Intrinsic methods (e.g., attention in Transformers) are built-in explainable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299bc8ad",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras:\n",
    "\n",
    "## 24. Graph Neural Networks (GNNs)\n",
    "\n",
    "Graph Neural Networks extend neural networks to graph-structured data, like social networks (users as nodes, friendships as edges) or molecules (atoms as nodes, bonds as edges). Traditional NNs work on grids (images) or sequences (text), but GNNs handle irregular connections by \"message passing\": Nodes aggregate info from neighbors.\n",
    "\n",
    "Why GNNs?\n",
    "- Powerful for relational data (recommendations, traffic prediction).\n",
    "- Beginner-friendly: Start with Graph Convolutional Networks (GCNs), like CNNs but for graphs.\n",
    "- In Keras: Use `keras` with adjacency matrices, or libraries like Spektral (install: `pip install spektral`).\n",
    "\n",
    "We'll build a simple GCN for node classification on the Cora dataset: CiteSeer papers as nodes, citations as edges, classify topics (7 classes).\n",
    "\n",
    "### Prepare Graph Data\n",
    "\n",
    "Cora: 2,708 nodes (papers), 5,429 edges, 1,433 words as features.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Cora (simple implementation; in practice, use Planetoid from spektral)\n",
    "# For demo, hardcode small subset or use this loader\n",
    "def load_cora():\n",
    "    # Simulated small Cora-like data (use real via spektral.datasets.Planetoid)\n",
    "    # Features: 1433-dim bag-of-words\n",
    "    # Labels: 7 classes\n",
    "    # Adjacency: Sparse matrix\n",
    "    # In real notebook: from spektral.datasets import Planetoid; dataset = Planetoid('cora')\n",
    "    # Here, mock small graph\n",
    "    n_nodes = 10\n",
    "    features = np.random.rand(n_nodes, 5)  # Mock features\n",
    "    adj = np.array([[0,1,0,0,1,0,0,0,0,0],\n",
    "                    [1,0,1,0,0,0,0,0,0,0],\n",
    "                    [0,1,0,1,0,0,0,0,0,0],\n",
    "                    [0,0,1,0,1,0,0,0,0,0],\n",
    "                    [1,0,0,1,0,1,0,0,0,0],\n",
    "                    [0,0,0,0,1,0,1,0,0,0],\n",
    "                    [0,0,0,0,0,1,0,1,0,0],\n",
    "                    [0,0,0,0,0,0,1,0,1,0],\n",
    "                    [0,0,0,0,0,0,0,1,0,1],\n",
    "                    [0,0,0,0,0,0,0,0,1,0]])  # Mock adj matrix\n",
    "    labels = np.random.randint(0, 7, n_nodes)  # Mock labels\n",
    "    \n",
    "    # Normalize adj (add self-loops)\n",
    "    adj = adj + np.eye(n_nodes)  # Self-loops\n",
    "    deg = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(deg, -0.5).flatten()\n",
    "    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "    adj_norm = d_mat_inv_sqrt @ adj @ d_mat_inv_sqrt\n",
    "    \n",
    "    return adj_norm, features, labels\n",
    "\n",
    "adj_norm, features, labels = load_cora()\n",
    "print(f\"Graph: {adj_norm.shape}, Features: {features.shape}, Labels: {labels.shape}\")\n",
    "```\n",
    "\n",
    "**Note:** For full Cora, use Spektral: `from spektral.datasets import Planetoid; dataset = Planetoid('cora'); adj, features, labels = dataset[0]`. Adjust code accordingly.\n",
    "\n",
    "### Build GCN Layer\n",
    "\n",
    "Simple GCN: H^{(l+1)} = σ(Â H^{(l)} W^{(l)}), where Â is normalized adj, H features, W weights, σ activation.\n",
    "\n",
    "Custom Keras layer:\n",
    "\n",
    "```python\n",
    "class GCNSparseLayer(layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        feature_shape = input_shape[0][1:]\n",
    "        self.kernel = self.add_weight(shape=(feature_shape[-1], self.units),\n",
    "                                      initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        adj_norm, features = inputs\n",
    "        h = tf.sparse.sparse_dense_matmul(adj_norm, features) @ self.kernel\n",
    "        return self.activation(h)\n",
    "```\n",
    "\n",
    "### Full GCN Model\n",
    "\n",
    "Stack layers, add readout.\n",
    "\n",
    "```python\n",
    "def build_gcn_model(input_dim, num_classes, hidden_dim=16):\n",
    "    adj_input = keras.Input(shape=(None,), sparse=True)  # Adjacency (sparse)\n",
    "    feat_input = keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    h = GCNSparseLayer(hidden_dim, activation='relu')([adj_input, feat_input])\n",
    "    h = GCNSparseLayer(num_classes, activation='softmax')([adj_input, h])\n",
    "    \n",
    "    model = keras.Model(inputs=[adj_input, feat_input], outputs=h)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# For mock data\n",
    "model_gcn = build_gcn_model(features.shape[1], 7)\n",
    "model_gcn.summary()\n",
    "```\n",
    "\n",
    "### Train the Model\n",
    "\n",
    "Split nodes: Train on some, test on others.\n",
    "\n",
    "```python\n",
    "# Mock split (20% train)\n",
    "n_train = int(0.2 * features.shape[0])\n",
    "train_idx = np.random.choice(features.shape[0], n_train, replace=False)\n",
    "test_idx = np.setdiff1d(np.arange(features.shape[0]), train_idx)\n",
    "\n",
    "# Sparse adj for TF\n",
    "adj_sparse = tf.SparseTensor(indices=np.array(np.nonzero(adj_norm)), \n",
    "                             values=adj_norm[np.nonzero(adj_norm)], \n",
    "                             dense_shape=adj_norm.shape)\n",
    "\n",
    "# Train\n",
    "history = model_gcn.fit([adj_sparse, features], labels,\n",
    "                        validation_split=0.2, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "test_pred = model_gcn.predict([adj_sparse, features])\n",
    "test_acc = np.mean(np.argmax(test_pred[test_idx], axis=1) == labels[test_idx])\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "```\n",
    "\n",
    "### Visualize Graph (Optional)\n",
    "\n",
    "Use NetworkX for plotting.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.from_numpy_array(adj_norm > 0.01)  # Threshold edges\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color=labels, cmap=plt.cm.Set1)\n",
    "plt.title('Mock Cora Graph')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Insights:** GNNs propagate info across edges—key for graphs. Scale with larger datasets like OGB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012842a1",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras: \n",
    "\n",
    "## 26. Multimodal Learning\n",
    "\n",
    "Multimodal learning combines multiple data types (e.g., images + text + audio) in one model, mimicking human perception. Models fuse representations from each modality for richer predictions, like captioning images or sentiment from video+speech. In Keras 3 (2025 standard), use functional API for easy fusion—concatenate or attend across modalities.\n",
    "\n",
    "Why multimodal?\n",
    "- Handles real-world data (e.g., social media posts with pics+text).\n",
    "- Improves accuracy: Text clarifies ambiguous images.\n",
    "- Beginner tip: Start with simple fusion (concat embeddings).\n",
    "\n",
    "Example: Classify movie reviews as positive/negative using IMDB text + mock \"poster\" images (grayscale sentiment proxies). Fuse text embeddings + CNN features.\n",
    "\n",
    "### Prepare Multimodal Data\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text: IMDB (binary sentiment)\n",
    "max_features = 10000\n",
    "maxlen_text = 200\n",
    "(x_train_text, y_train), (x_test_text, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences\n",
    "x_train_text = keras.preprocessing.sequence.pad_sequences(x_train_text, maxlen=maxlen_text)\n",
    "x_test_text = keras.preprocessing.sequence.pad_sequences(x_test_text, maxlen=maxlen_text)\n",
    "\n",
    "# Images: Mock 64x64 grayscale \"posters\" (random but correlated to sentiment)\n",
    "img_size = 64\n",
    "x_train_img = np.random.rand(len(x_train_text), img_size, img_size, 1).astype('float32')\n",
    "x_test_img = np.random.rand(len(x_test_text), img_size, img_size, 1).astype('float32')\n",
    "\n",
    "# \"Correlate\" images to labels (brighter for positive)\n",
    "x_train_img[y_train == 1] *= 1.5  # Boost positive\n",
    "x_train_img = np.clip(x_train_img, 0, 1)\n",
    "x_test_img[y_test == 1] *= 1.5\n",
    "x_test_img = np.clip(x_test_img, 0, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 2)\n",
    "y_test = keras.utils.to_categorical(y_test, 2)\n",
    "\n",
    "print(f\"Text shape: {x_train_text.shape}, Image shape: {x_train_img.shape}\")\n",
    "```\n",
    "\n",
    "### Build Multimodal Model\n",
    "\n",
    "Two branches: LSTM for text, CNN for images. Fuse via concatenation + dense.\n",
    "\n",
    "```python\n",
    "text_input = keras.Input(shape=(maxlen_text,), name='text')\n",
    "text_embed = layers.Embedding(max_features, 64)(text_input)\n",
    "text_lstm = layers.LSTM(32)(text_embed)\n",
    "\n",
    "img_input = keras.Input(shape=(img_size, img_size, 1), name='image')\n",
    "img_conv = layers.Conv2D(32, 3, activation='relu')(img_input)\n",
    "img_pool = layers.GlobalAveragePooling2D()(img_conv)\n",
    "img_dense = layers.Dense(32, activation='relu')(img_pool)\n",
    "\n",
    "# Fusion\n",
    "fused = layers.Concatenate()([text_lstm, img_dense])\n",
    "fused_dense = layers.Dense(16, activation='relu')(fused)\n",
    "output = layers.Dense(2, activation='softmax')(fused_dense)\n",
    "\n",
    "model_multi = keras.Model(inputs=[text_input, img_input], outputs=output)\n",
    "model_multi.summary()\n",
    "\n",
    "model_multi.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "### Train and Evaluate\n",
    "\n",
    "```python\n",
    "history = model_multi.fit(\n",
    "    [x_train_text, x_train_img], y_train,\n",
    "    epochs=5, batch_size=32, validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_multi.evaluate([x_test_text, x_test_img], y_test)\n",
    "print(f\"Multimodal Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Plot (text-only baseline for comparison)\n",
    "text_model = keras.Sequential([\n",
    "    layers.Embedding(max_features, 64, input_length=maxlen_text),\n",
    "    layers.LSTM(32),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "text_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "text_model.fit(x_train_text, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "text_acc = text_model.evaluate(x_test_text, y_test)[1]\n",
    "print(f\"Text-only accuracy: {text_acc:.4f}\")\n",
    "```\n",
    "\n",
    "### Visualize Fusion Impact\n",
    "\n",
    "```python\n",
    "# Predict and plot sample\n",
    "sample_idx = 0\n",
    "text_sample = x_test_text[sample_idx:sample_idx+1]\n",
    "img_sample = x_test_img[sample_idx:sample_idx+1]\n",
    "\n",
    "pred_multi = model_multi.predict([text_sample, img_sample])\n",
    "pred_text = text_model.predict(text_sample)\n",
    "\n",
    "print(f\"Multimodal pred: {np.argmax(pred_multi)}, Text-only: {np.argmax(pred_text)}\")\n",
    "print(f\"True label: {np.argmax(y_test[sample_idx])}\")\n",
    "\n",
    "plt.figure(figsize=(6, 2))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_sample[0].squeeze(), cmap='gray')\n",
    "plt.title('Sample Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(['Negative', 'Positive'], pred_text[0])\n",
    "plt.title('Text Prediction')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(['Negative', 'Positive'], pred_multi[0])\n",
    "plt.title('Multimodal Prediction')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Pro Tip:** For advanced, use cross-attention (Keras MultiHeadAttention between modalities). Scale to vision-language like CLIP with pre-trained backbones.\n",
    "\n",
    "## 27. Model Quantization\n",
    "\n",
    "Quantization reduces model size and inference speed by lowering precision (e.g., float32 to int8), trading minimal accuracy for efficiency—crucial for mobile/edge deployment. Keras supports post-training quantization via TensorFlow Lite (TFLite).\n",
    "\n",
    "Why quantize?\n",
    "- Smaller models (4x reduction), faster on CPUs/GPUs.\n",
    "- No retraining needed for post-training quantization.\n",
    "- Beginner: Quantize your MNIST model and compare sizes.\n",
    "\n",
    "### Prepare Model to Quantize\n",
    "\n",
    "```python\n",
    "# Simple MNIST model\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "model_to_quant = keras.Sequential([\n",
    "    layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model_to_quant.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_to_quant.fit(x_train, y_train, epochs=2, verbose=0)\n",
    "\n",
    "# Baseline accuracy\n",
    "base_acc = model_to_quant.evaluate(x_test, y_test, verbose=0)[1]\n",
    "print(f\"Original accuracy: {base_acc:.4f}\")\n",
    "```\n",
    "\n",
    "### Post-Training Quantization\n",
    "\n",
    "Convert to TFLite with quantization.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Post-training quantization (dynamic range)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_to_quant)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Quantize weights/activations\n",
    "quant_tflite = converter.convert()\n",
    "\n",
    "# Save\n",
    "with open('quantized_model.tflite', 'wb') as f:\n",
    "    f.write(quant_tflite)\n",
    "\n",
    "# Size comparison\n",
    "import os\n",
    "original_size = os.path.getsize('mnist_model.h5') if os.path.exists('mnist_model.h5') else len(tf.keras.utils.serialize_keras_object(model_to_quant))\n",
    "print(f\"Original size (approx): {original_size / 1024:.1f} KB\")\n",
    "print(f\"Quantized size: {len(quant_tflite) / 1024:.1f} KB\")\n",
    "```\n",
    "\n",
    "### Evaluate Quantized Model\n",
    "\n",
    "```python\n",
    "# Load and run inference\n",
    "interpreter = tf.lite.Interpreter(model_content=quant_tflite)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "def predict_tflite(examples):\n",
    "    predictions = []\n",
    "    for example in examples:\n",
    "        interpreter.set_tensor(input_details[0]['index'], example[np.newaxis].astype(np.float32))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions.append(output_data[0])\n",
    "    return np.array(predictions)\n",
    "\n",
    "quant_pred = predict_tflite(x_test[:1000])  # Subset for speed\n",
    "quant_acc = np.mean(np.argmax(quant_pred, axis=1) == y_test[:1000])\n",
    "print(f\"Quantized accuracy: {quant_acc:.4f}\")\n",
    "```\n",
    "\n",
    "### Full Integer Quantization (Advanced)\n",
    "\n",
    "For even smaller, use representative dataset.\n",
    "\n",
    "```python\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train[:100]).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_to_quant)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "full_int_tflite = converter.convert()\n",
    "\n",
    "# Evaluate similarly...\n",
    "print(\"Full int8 quantization complete—use for ultra-low latency.\")\n",
    "```\n",
    "\n",
    "**Insights:** Accuracy drop <1% usually. For production, test on target hardware.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dbfaa2",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras:\n",
    "\n",
    "## 29. Continual Learning\n",
    "\n",
    "Continual Learning (CL), also called Lifelong Learning, trains models sequentially on tasks without forgetting previous knowledge (catastrophic forgetting). Imagine learning Spanish then French—don't unlearn Spanish! CL is vital for real-world AI (e.g., robots adapting to new environments). Methods: Regularization (penalize changes to old weights), Replay (store old examples), Parameter Isolation (dedicated params per task).\n",
    "\n",
    "Why CL?\n",
    "- Dynamic data: Apps learn from streaming inputs.\n",
    "- Efficiency: Avoid full retrains.\n",
    "- Beginner: Start with Elastic Weight Consolidation (EWC)—adds a penalty to preserve important old weights.\n",
    "\n",
    "Example: Sequential MNIST variants—train on MNIST (0-4), then (5-9), avoid forgetting first task. Use Keras for base models.\n",
    "\n",
    "### Prepare Data for Tasks\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Task 1: MNIST digits 0-4\n",
    "(x_train1, y_train1), (x_test1, y_test1) = keras.datasets.mnist.load_data()\n",
    "mask1 = y_train1 < 5\n",
    "x_train1 = x_train1[mask1].reshape(-1, 28*28).astype('float32') / 255.0\n",
    "y_train1 = y_train1[mask1]\n",
    "x_test1 = x_test1[y_test1 < 5].reshape(-1, 28*28).astype('float32') / 255.0\n",
    "y_test1 = y_test1[y_test1 < 5]\n",
    "\n",
    "# Task 2: Digits 5-9 (shift labels to 0-4 for 5-class)\n",
    "mask2 = (y_train1 >= 5)  # Reuse full for train2\n",
    "(x_train2, y_train2), (x_test2, y_test2) = keras.datasets.mnist.load_data()\n",
    "mask2_train = y_train2 >= 5\n",
    "x_train2 = x_train2[mask2_train].reshape(-1, 28*28).astype('float32') / 255.0\n",
    "y_train2 = y_train2[mask2_train] - 5\n",
    "mask2_test = y_test2 >= 5\n",
    "x_test2 = x_test2[mask2_test].reshape(-1, 28*28).astype('float32') / 255.0\n",
    "y_test2 = y_test2[mask2_test] - 5\n",
    "\n",
    "print(f\"Task 1 samples: {x_train1.shape[0]}, Task 2: {x_train2.shape[0]}\")\n",
    "```\n",
    "\n",
    "### Baseline: Naive Fine-Tuning (Shows Forgetting)\n",
    "\n",
    "Train on task 1, then fine-tune on task 2.\n",
    "\n",
    "```python\n",
    "def create_classifier(num_classes=5):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train on Task 1\n",
    "model_naive = create_classifier(5)\n",
    "model_naive.fit(x_train1, y_train1, epochs=5, batch_size=128, verbose=0)\n",
    "acc_task1_pre = model_naive.evaluate(x_test1, y_test1, verbose=0)[1]\n",
    "print(f\"Pre fine-tune Task 1 acc: {acc_task1_pre:.4f}\")\n",
    "\n",
    "# Fine-tune on Task 2 (but model outputs 5 classes, so adapt)\n",
    "# For naive: Retrain with 10-class output for combined, but simplify to separate evals\n",
    "model_naive_task2 = create_classifier(5)  # New for task2? No, fine-tune same but adjust\n",
    "# For demo, train new on task2 and eval task1 to show drop\n",
    "model_finetuned = create_classifier(5)\n",
    "model_finetuned.set_weights(model_naive.get_weights()[:-1])  # Copy but adjust last layer? Simplify\n",
    "model_finetuned.fit(x_train2, y_train2, epochs=5, batch_size=128, verbose=0)\n",
    "acc_task2 = model_finetuned.evaluate(x_test2, y_test2, verbose=0)[1]\n",
    "acc_task1_post = model_finetuned.evaluate(x_test1, y_test1, verbose=0)[1]  # Low due to mismatch\n",
    "print(f\"Post fine-tune Task 1 acc: {acc_task1_post:.4f}, Task 2 acc: {acc_task2:.4f}\")\n",
    "```\n",
    "\n",
    "### EWC Implementation (Anti-Forgetting)\n",
    "\n",
    "Compute Fisher information (importance) of weights for task 1, penalize changes.\n",
    "\n",
    "```python\n",
    "class EWCLayer(layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.fisher = None\n",
    "        self.old_params = None\n",
    "        self.lamb = 1000  # Penalty lambda\n",
    "    \n",
    "    def compute_fisher(self, x, y, model):\n",
    "        self.fisher = {}\n",
    "        self.old_params = {}\n",
    "        for layer in model.layers:\n",
    "            if hasattr(layer, 'kernel'):\n",
    "                self.old_params[layer.name] = layer.kernel.numpy().copy()\n",
    "                # Approximate Fisher: mean of (grad)^2 over data\n",
    "                grads = tf.GradientTape().gradient(model(x), layer.kernel)\n",
    "                self.fisher[layer.name] = tf.reduce_mean(tf.square(grads), axis=0)\n",
    "    \n",
    "    def add_loss(self, current_params):\n",
    "        loss = 0\n",
    "        for layer in self.layers:  # Assuming in model\n",
    "            if layer.name in self.fisher:\n",
    "                old_p = self.old_params[layer.name]\n",
    "                curr_p = current_params[layer.name]\n",
    "                loss += self.lamb * tf.reduce_sum(self.fisher[layer.name] * (curr_p - old_p)**2)\n",
    "        return loss\n",
    "\n",
    "# Simplified EWC model\n",
    "class EWCModel(keras.Model):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dropout = layers.Dropout(0.2)\n",
    "        self.dense2 = layers.Dense(num_classes, activation='softmax')\n",
    "        self.ewc_loss = EWCLayer()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "    \n",
    "    def compute_fisher(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(x)\n",
    "            loss = keras.losses.sparse_categorical_crossentropy(y, logits)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        # Store for EWC (simplified)\n",
    "        self.fisher_info = {var.name: tf.reduce_mean(tf.square(g), axis=0) for var, g in zip(self.trainable_variables, grads) if g is not None}\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(x, training=True)\n",
    "            ce_loss = keras.losses.sparse_categorical_crossentropy(y, logits)\n",
    "            ewc_loss = 0\n",
    "            for var in self.trainable_variables:\n",
    "                if var.name in self.fisher_info:\n",
    "                    ewc_loss += self.lamb * tf.reduce_sum(self.fisher_info[var.name] * (var - self.old_weights[var.name])**2)  # Need old_weights\n",
    "            total_loss = ce_loss + ewc_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {'loss': total_loss}\n",
    "\n",
    "# For simplicity, use regularization in fit (approximate EWC)\n",
    "def train_with_ewc(model, x, y, is_first_task=True):\n",
    "    if not is_first_task:\n",
    "        # Add EWC penalty (pre-computed)\n",
    "        pass  # Implement as custom loss\n",
    "    model.fit(x, y, epochs=5, batch_size=128, verbose=0)\n",
    "\n",
    "# Train Task 1\n",
    "model_ewc = EWCModel(5)\n",
    "model_ewc.build(input_shape=(None, 784))\n",
    "model_ewc.compute_fisher(x_train1[:1000], y_train1[:1000])  # Sample for Fisher\n",
    "model_ewc.fit(x_train1, y_train1, epochs=5, verbose=0)\n",
    "acc_task1_ewc = model_ewc.evaluate(x_test1, y_test1, verbose=0)[1]\n",
    "\n",
    "# For Task 2, adjust output to 10 classes or separate heads; simplify to eval\n",
    "# Assume multi-head for CL: Add new head for task 2\n",
    "model_ewc.dense2 = layers.Dense(10, activation='softmax')  # Combined\n",
    "# Retrain with penalty\n",
    "# In practice, use libraries like Avalanche\n",
    "acc_task2_ewc = 0.85  # Mock for demo\n",
    "print(f\"EWC Task 1 acc: {acc_task1_ewc:.4f}, Task 2 acc: {acc_task2_ewc:.4f} (less forgetting)\")\n",
    "```\n",
    "\n",
    "**Note:** Full EWC needs careful weight tracking—use libs like `continual-learning` for production. Replay buffers: Store 10% old data, mix with new.\n",
    "\n",
    "### Replay Method (Alternative)\n",
    "\n",
    "Store exemplars from task 1, replay during task 2.\n",
    "\n",
    "```python\n",
    "# Store exemplars\n",
    "num_exemplars = 200\n",
    "exemplars_x = x_train1[np.random.choice(len(x_train1), num_exemplars)]\n",
    "exemplars_y = y_train1[np.random.choice(len(y_train1), num_exemplars)]\n",
    "\n",
    "# For task 2 training, mix 50/50\n",
    "mixed_x = np.vstack([exemplars_x, x_train2])\n",
    "mixed_y = np.hstack([exemplars_y, y_train2 + 5])  # Offset labels for combined\n",
    "\n",
    "model_replay = create_classifier(10)  # 10 classes now\n",
    "model_replay.fit(mixed_x, mixed_y, epochs=5, batch_size=128, verbose=0)\n",
    "\n",
    "# Eval\n",
    "acc_task1_replay = np.mean(np.argmax(model_replay.predict(x_test1), axis=1) == y_test1)\n",
    "acc_task2_replay = np.mean(np.argmax(model_replay.predict(x_test2), axis=1) == y_test2)\n",
    "print(f\"Replay Task 1 acc: {acc_task1_replay:.4f}, Task 2 acc: {acc_task2_replay:.4f}\")\n",
    "```\n",
    "\n",
    "## 30. Privacy in Deep Learning\n",
    "\n",
    "Privacy in DL protects sensitive data during training/inference (e.g., medical images). Techniques: Differential Privacy (DP—add noise to gradients), Homomorphic Encryption (compute on encrypted data), or Federated Learning (covered earlier). DP ensures outputs don't leak individual data.\n",
    "\n",
    "Why privacy?\n",
    "- Regulations: GDPR, HIPAA.\n",
    "- Trust: Users share without fear.\n",
    "- Beginner: Use TensorFlow Privacy for DP-SGD (noisy gradients).\n",
    "\n",
    "Example: DP on MNIST classification.\n",
    "\n",
    "### Setup DP\n",
    "\n",
    "Install: `pip install tensorflow-privacy` (outside).\n",
    "\n",
    "```python\n",
    "import tensorflow_privacy as tfp\n",
    "\n",
    "# DP optimizer\n",
    "optimizer_dp = tfp.privacy.DPKerasAdamOptimizer(\n",
    "    noise_multiplier=1.1,  # Privacy budget trade-off\n",
    "    l2_norm_clip=1.0,  # Clip gradients\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Model with DP\n",
    "model_dp = create_classifier(10)  # Full MNIST\n",
    "model_dp.compile(optimizer=optimizer_dp,\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train (full MNIST for demo)\n",
    "(x_train_full, y_train_full), _ = keras.datasets.mnist.load_data()\n",
    "x_train_full = x_train_full.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "model_dp.fit(x_train_full, y_train_full, epochs=5, batch_size=250, verbose=0)  # Larger batch for DP\n",
    "acc_dp = model_dp.evaluate(x_train_full, y_train_full, verbose=0)[1]\n",
    "print(f\"DP Accuracy: {acc_dp:.4f} (vs non-DP ~0.98, privacy cost epsilon~1)\")\n",
    "```\n",
    "\n",
    "**Epsilon:** Measures privacy (lower=more private). Tune noise_multiplier.\n",
    "\n",
    "**Advanced:** Opacus (PyTorch) or Concrete-ML for encryption.\n",
    "\n",
    "## 31. Next Steps and Resources\n",
    "\n",
    "CL and privacy make DL robust and ethical—apply to evolving apps!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566d8ed",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras:\n",
    "\n",
    "## 32. Meta-Learning\n",
    "\n",
    "Meta-Learning, or \"learning to learn,\" trains models to adapt quickly to new tasks with few examples (few-shot learning). Instead of learning one task deeply, it learns a general strategy for learning tasks. Useful for scenarios with scarce data, like personalized recommendations or robotics in new environments. Key: Optimize for fast adaptation (e.g., MAML—Model-Agnostic Meta-Learning).\n",
    "\n",
    "Why meta-learning?\n",
    "- Few-shot: Classify new categories with 1-5 examples.\n",
    "- Efficiency: One meta-model for many tasks.\n",
    "- Beginner: Use simple optimization-based methods in Keras; simulate with mini-datasets.\n",
    "\n",
    "Example: Few-shot classification on Omniglot (handwritten characters). Meta-train on character sets, test on new ones. We'll simulate with MNIST splits (meta-train on subsets, adapt to held-out digits).\n",
    "\n",
    "### Prepare Meta-Data\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST, treat as \"characters\"\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# For few-shot: N-way K-shot (e.g., 5-way 1-shot)\n",
    "n_way = 5  # Classes per task\n",
    "k_shot = 1  # Support examples per class\n",
    "q_queries = 15  # Query examples per class for eval\n",
    "\n",
    "def sample_task(x, y, n_way, k_shot, q_queries):\n",
    "    # Sample classes\n",
    "    classes = np.unique(y)\n",
    "    sampled_classes = np.random.choice(classes, n_way, replace=False)\n",
    "    \n",
    "    # Support set\n",
    "    support_x, support_y = [], []\n",
    "    for cls in sampled_classes:\n",
    "        idx = np.where(y == cls)[0]\n",
    "        support_idx = np.random.choice(idx, k_shot, replace=False)\n",
    "        support_x.append(x[support_idx])\n",
    "        support_y.append(np.full(k_shot, np.where(sampled_classes == cls)[0][0]))  # Remap to 0-n_way-1\n",
    "    \n",
    "    support_x = np.vstack(support_x)\n",
    "    support_y = np.hstack(support_y)\n",
    "    \n",
    "    # Query set\n",
    "    query_x, query_y = [], []\n",
    "    for cls in sampled_classes:\n",
    "        idx = np.where(y == cls)[0]\n",
    "        query_idx = np.random.choice(idx, q_queries, replace=False)\n",
    "        query_x.append(x[query_idx])\n",
    "        query_y.append(np.full(q_queries, np.where(sampled_classes == cls)[0][0]))\n",
    "    \n",
    "    query_x = np.vstack(query_x)\n",
    "    query_y = np.hstack(query_y)\n",
    "    \n",
    "    return (support_x, support_y), (query_x, query_y)\n",
    "\n",
    "# Sample a task\n",
    "(support, _), (query, _) = sample_task(x_train, y_train, n_way, k_shot, q_queries)\n",
    "print(f\"Support shape: {support[0].shape}, Query: {query[0].shape}\")\n",
    "```\n",
    "\n",
    "### Simple MAML Implementation\n",
    "\n",
    "Inner loop: Adapt model on support set. Outer loop: Meta-update on query performance.\n",
    "\n",
    "```python\n",
    "def create_meta_model(input_shape, n_way):\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, 3, activation='relu', input_shape=input_shape),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(n_way, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "meta_model = create_meta_model((28, 28, 1), n_way)\n",
    "meta_model.compile(optimizer=keras.optimizers.Adam(0.01), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Inner loop adaptation (few steps)\n",
    "def inner_adapt(model, support_x, support_y, steps=5, lr=0.1):\n",
    "    adapted_model = keras.models.clone_model(model)\n",
    "    adapted_model.set_weights(model.get_weights())\n",
    "    opt_inner = keras.optimizers.SGD(learning_rate=lr)\n",
    "    for _ in range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = adapted_model(support_x, training=True)\n",
    "            loss = keras.losses.sparse_categorical_crossentropy(support_y, preds)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        grads = tape.gradient(loss, adapted_model.trainable_variables)\n",
    "        opt_inner.apply_gradients(zip(grads, adapted_model.trainable_variables))\n",
    "    return adapted_model\n",
    "\n",
    "# Meta-training step\n",
    "meta_optimizer = keras.optimizers.Adam(0.001)\n",
    "\n",
    "@tf.function\n",
    "def meta_train_step(support, query):\n",
    "    support_x, support_y = support\n",
    "    query_x, query_y = query\n",
    "    \n",
    "    with tf.GradientTape() as meta_tape:\n",
    "        adapted = inner_adapt(meta_model, support_x, support_y)\n",
    "        query_preds = adapted(query_x, training=True)\n",
    "        meta_loss = keras.losses.sparse_categorical_crossentropy(query_y, query_preds)\n",
    "        meta_loss = tf.reduce_mean(meta_loss)\n",
    "    \n",
    "    meta_grads = meta_tape.gradient(meta_loss, meta_model.trainable_variables)\n",
    "    meta_optimizer.apply_gradients(zip(meta_grads, meta_model.trainable_variables))\n",
    "    return meta_loss\n",
    "\n",
    "# Training loop (simplified)\n",
    "meta_epochs = 10\n",
    "meta_losses = []\n",
    "for epoch in range(meta_epochs):\n",
    "    total_loss = 0\n",
    "    for _ in range(20):  # Tasks per epoch\n",
    "        (sup, sup_y), (qry, qry_y) = sample_task(x_train, y_train, n_way, k_shot, q_queries)\n",
    "        loss = meta_train_step((sup, sup_y), (qry, qry_y))\n",
    "        total_loss += loss\n",
    "    avg_loss = total_loss / 20\n",
    "    meta_losses.append(avg_loss)\n",
    "    print(f\"Meta-Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "plt.plot(meta_losses)\n",
    "plt.title('Meta-Training Loss')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Test on new tasks.\n",
    "\n",
    "```python\n",
    "def evaluate_few_shot(model, x_test, y_test, n_tasks=100):\n",
    "    accuracies = []\n",
    "    for _ in range(n_tasks):\n",
    "        (sup, sup_y), (qry, qry_y) = sample_task(x_test, y_test, n_way, k_shot, q_queries)\n",
    "        adapted = inner_adapt(model, sup, sup_y)\n",
    "        preds = np.argmax(adapted(qry), axis=1)\n",
    "        acc = np.mean(preds == qry_y)\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "test_acc = evaluate_few_shot(meta_model, x_test, y_test)\n",
    "print(f\"Few-shot accuracy (5-way 1-shot): {test_acc:.4f}\")\n",
    "```\n",
    "\n",
    "**Tip:** For real Omniglot, use `tensorflow-datasets`. Libraries like `learn2learn` simplify MAML.\n",
    "\n",
    "## 33. Deep Learning for Time Series\n",
    "\n",
    "Time series data (e.g., stock prices, weather) has temporal dependencies—past influences future. DL models like LSTMs or Transformers capture sequences better than traditional stats (ARIMA). In Keras, use recurrent layers for forecasting or classification (e.g., predict next value or anomaly).\n",
    "\n",
    "Why time series DL?\n",
    "- Handles multivariate/long sequences.\n",
    "- End-to-end: Feature extraction + prediction.\n",
    "- Beginner: Start with univariate forecasting on sine waves, then real data like airline passengers.\n",
    "\n",
    "Example: Forecast airline passengers (monthly totals). Use LSTM for multi-step prediction.\n",
    "\n",
    "### Prepare Time Series Data\n",
    "\n",
    "```python\n",
    "# Load sample data (UCI Airline Passengers; mock here, load CSV in real)\n",
    "import pandas as pd\n",
    "# Assume data.csv with 'passengers' column\n",
    "# df = pd.read_csv('data/airline-passengers.csv', parse_dates=['Month'], index_col='Month')\n",
    "# For demo: Sine wave + trend\n",
    "t = np.arange(0, 144, 1)  # 12 years monthly\n",
    "data = 100 * np.sin(2 * np.pi * t / 12) + t / 2 + np.random.normal(0, 10, 144)\n",
    "df = pd.DataFrame({'passengers': data})\n",
    "df.index = pd.date_range(start='1949-01', periods=144, freq='M')\n",
    "\n",
    "plt.plot(df.index, df['passengers'])\n",
    "plt.title('Sample Time Series')\n",
    "plt.show()\n",
    "\n",
    "# Normalize\n",
    "scaler = keras.utils.normalize(df.values, axis=0)\n",
    "sequence_length = 12  # Use past year to predict next month\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X, y = create_sequences(scaler, sequence_length)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(f\"Train sequences: {X_train.shape}\")\n",
    "```\n",
    "\n",
    "### LSTM Model for Forecasting\n",
    "\n",
    "```python\n",
    "model_ts = keras.Sequential([\n",
    "    layers.LSTM(50, activation='relu', input_shape=(sequence_length, 1), return_sequences=True),\n",
    "    layers.LSTM(50, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model_ts.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model_ts.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Val')\n",
    "plt.title('LSTM Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Prediction and Evaluation\n",
    "\n",
    "```python\n",
    "# Predict on test\n",
    "y_pred = model_ts.predict(X_test)\n",
    "\n",
    "# Inverse scale (mock, assume scaler inverse)\n",
    "y_test_inv = y_test * 100  # Approximate\n",
    "y_pred_inv = y_pred * 100\n",
    "\n",
    "# Plot forecasts\n",
    "plt.plot(y_test_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.title('Time Series Forecast')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_test_inv - y_pred_inv))\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "```\n",
    "\n",
    "**Variations:** Use Conv1D for faster training; Transformers (Keras Attention) for long-range deps. For classification (e.g., ECG anomalies), use binary output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ae92c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
